{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a083a350",
      "metadata": {},
      "source": [
        "# Skills: Apple Human Interface Guidelines\n",
        "ðŸ”— url: https://developer.apple.com/design/human-interface-guidelines\n",
        "\n",
        "Abbreviations:\n",
        "- Human Interface Guidelines (HIG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbbe1712",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "16ad4bd6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "import requests\n",
        "from tqdm.auto import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from markdownify import markdownify as md\n",
        "from playwright.async_api import async_playwright"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8efaadee",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = Path(\"./data/apple_hig\")\n",
        "data_dir.mkdir(exist_ok=True, parents=True)\n",
        "output_dir = Path(\"../skills/apple_hig\")\n",
        "output_dir.mkdir(exist_ok=True, parents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6267c842",
      "metadata": {},
      "source": [
        "## Scrape Apple HIG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ca51b2d",
      "metadata": {},
      "source": [
        "### Extract URLs to all pages with structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6a00123e",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def get_hig_pages(\n",
        "    base_url: str = \"https://developer.apple.com/design/human-interface-guidelines\",\n",
        ") -> dict[str, Any]:\n",
        "    \"\"\"Crawl Apple HIG and return a structured map of all pages.\"\"\"\n",
        "\n",
        "    async with async_playwright() as pw:\n",
        "        browser = await pw.chromium.launch(headless=True)\n",
        "        # context = await browser.new_context(viewport={\"width\": 1920, \"height\": 1080})\n",
        "        # page = await context.new_page()\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        await page.goto(base_url, wait_until=\"networkidle\")\n",
        "        await page.wait_for_selector(\".navigator-card-item\")\n",
        "\n",
        "        origin = f\"{urlparse(page.url).scheme}://{urlparse(page.url).netloc}\"\n",
        "\n",
        "        # Step 1: Get 6 main sections from sidebar navigator\n",
        "        navigator = page.locator(\".navigator\")\n",
        "        items = await navigator.locator(\".navigator-card-item:has(.tree-toggle)\").all()\n",
        "\n",
        "        sections = []\n",
        "        for item in items:\n",
        "            link = item.locator(\".leaf-link\")\n",
        "            name = (await link.text_content()).strip()\n",
        "            url = f\"{origin}{await link.get_attribute('href')}\"\n",
        "            sections.append({\"name\": name, \"url\": url})\n",
        "\n",
        "        print(f\"Found {len(sections)} sections: {[s['name'] for s in sections]}\\n\")\n",
        "\n",
        "        async def get_card_links(url: str) -> list[dict]:\n",
        "            \"\"\"Helper: Navigate to a URL and return all HIG links from its card grid.\"\"\"\n",
        "            await page.goto(url, wait_until=\"networkidle\")\n",
        "            await page.wait_for_selector(\".TopicsLinkCardGrid\")\n",
        "\n",
        "            card_grid = page.locator(\".TopicsLinkCardGrid\")\n",
        "\n",
        "            if await card_grid.count() == 0:\n",
        "                return []\n",
        "\n",
        "            links = await card_grid.locator(\n",
        "                \"a[href*='design/human-interface-guidelines']\"\n",
        "            ).all()\n",
        "\n",
        "            results = []\n",
        "            for link in links:\n",
        "                name = (await link.text_content()).strip()\n",
        "                href = await link.get_attribute(\"href\")\n",
        "                results.append({\"name\": name, \"url\": f\"{origin}{href}\"})\n",
        "\n",
        "            return results\n",
        "\n",
        "        async def is_subsection(url: str) -> bool:\n",
        "            \"\"\"Helper: Navigate to a URL and check if it's a sub-section listing page.\"\"\"\n",
        "            await page.goto(url, wait_until=\"networkidle\")\n",
        "            await page.wait_for_selector(\".doc-content\")\n",
        "            h2_count = await page.locator(\".doc-content h2\").count()\n",
        "            return h2_count == 0\n",
        "\n",
        "        # Step 2: For each section, get children and classify them\n",
        "        hig_structure = {}\n",
        "\n",
        "        for section in sections:\n",
        "            section_slug = section[\"url\"].rstrip(\"/\").split(\"/\")[-1]\n",
        "            print(f\"Section: {section['name']}\")\n",
        "\n",
        "            children = await get_card_links(section[\"url\"])\n",
        "            entry = {\"url\": section[\"url\"], \"pages\": [], \"subsections\": {}}\n",
        "\n",
        "            for child in children:\n",
        "                child_slug = child[\"url\"].rstrip(\"/\").split(\"/\")[-1]\n",
        "\n",
        "                if await is_subsection(child[\"url\"]):\n",
        "                    # Step 2.1: Sub-section found, get its pages\n",
        "                    sub_pages = await get_card_links(child[\"url\"])\n",
        "                    entry[\"subsections\"][child_slug] = {\n",
        "                        \"url\": child[\"url\"],\n",
        "                        \"pages\": [p[\"url\"] for p in sub_pages],\n",
        "                    }\n",
        "                else:\n",
        "                    entry[\"pages\"].append(child[\"url\"])\n",
        "\n",
        "            total = len(entry[\"pages\"]) + sum(\n",
        "                len(s[\"pages\"]) for s in entry[\"subsections\"].values()\n",
        "            )\n",
        "            print(\n",
        "                f\"  -> {len(entry['pages'])} pages, \"\n",
        "                f\"{len(entry['subsections'])} sub-sections, \"\n",
        "                f\"{total} total\\n\"\n",
        "            )\n",
        "            hig_structure[section_slug] = entry\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "    return hig_structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "34721e4a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6 sections: ['Getting started', 'Foundations', 'Patterns', 'Components', 'Inputs', 'Technologies']\n",
            "\n",
            "Section: Getting started\n",
            "  -> 7 pages, 0 sub-sections, 7 total\n",
            "\n",
            "Section: Foundations\n",
            "  -> 18 pages, 0 sub-sections, 18 total\n",
            "\n",
            "Section: Patterns\n",
            "  -> 25 pages, 0 sub-sections, 25 total\n",
            "\n",
            "Section: Components\n",
            "  -> 0 pages, 8 sub-sections, 63 total\n",
            "\n",
            "Section: Inputs\n",
            "  -> 13 pages, 0 sub-sections, 13 total\n",
            "\n",
            "Section: Technologies\n",
            "  -> 29 pages, 0 sub-sections, 29 total\n",
            "\n"
          ]
        }
      ],
      "source": [
        "hig_structure = await get_hig_pages()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f0fd6e56",
      "metadata": {},
      "outputs": [],
      "source": [
        "hig_structure_path = data_dir / \"hig_structure.json\"\n",
        "with open(hig_structure_path, \"w\") as f:\n",
        "    json.dump(hig_structure, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b1ce19a",
      "metadata": {},
      "source": [
        "### Scrape content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "0f4d85e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def flatten_hig_structure(hig_structure: dict[str, Any]) -> list[str]:\n",
        "    all_pages = []\n",
        "    for section in hig_structure.values():\n",
        "        all_pages.extend(section[\"pages\"])\n",
        "\n",
        "        for sub in section[\"subsections\"].values():\n",
        "            all_pages.extend(sub[\"pages\"])\n",
        "\n",
        "    return all_pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bae68a1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def scrape_hig_content(hig_structure: dict[str, Any]) -> list[dict]:\n",
        "    \"\"\"Scrape markdown content from all HIG pages.\"\"\"\n",
        "\n",
        "    all_pages = flatten_hig_structure(hig_structure)\n",
        "\n",
        "    print(f\"Scraping {len(all_pages)} pages...\\n\")\n",
        "\n",
        "    async with async_playwright() as pw:\n",
        "        browser = await pw.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for url in tqdm(all_pages, desc=\"Scraping pages\", unit=\"page\"):\n",
        "            try:\n",
        "                await page.goto(url, wait_until=\"networkidle\")\n",
        "                await page.wait_for_selector(\".doc-content\")\n",
        "\n",
        "                title = (await page.locator(\"h1\").first.text_content()).strip()\n",
        "                abstract = \"\"\n",
        "                abstract_loc = page.locator(\".abstract\")\n",
        "                if await abstract_loc.count() > 0:\n",
        "                    abstract = (await abstract_loc.first.text_content()).strip()\n",
        "\n",
        "                content_html = await page.locator(\".doc-content\").inner_html()\n",
        "                markdown_content = md(content_html, heading_style=\"ATX\")\n",
        "                full_content = (\n",
        "                    f\"# {title}\\n\\n{abstract}\\n\\n{markdown_content}\"\n",
        "                    if abstract\n",
        "                    else f\"# {title}\\n\\n{markdown_content}\"\n",
        "                )\n",
        "\n",
        "                results[url] = full_content\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error scraping {url}: {e}\")\n",
        "                continue\n",
        "\n",
        "        await page.close()\n",
        "        await browser.close()\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "4745718a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping 155 pages...\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4f2b7b76ca04de7b7d44e1cb679077e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Scraping pages:   0%|          | 0/155 [00:00<?, ?page/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Future exception was never retrieved\n",
            "future: <Future finished exception=TargetClosedError('Target page, context or browser has been closed')>\n",
            "playwright._impl._errors.TargetClosedError: Target page, context or browser has been closed\n"
          ]
        }
      ],
      "source": [
        "scraped_pages = await scrape_hig_content(hig_structure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "8bda41a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "scraped_pages_path = data_dir / \"scraped_pages.json\"\n",
        "with open(scraped_pages_path, \"w\") as f:\n",
        "    json.dump(scraped_pages, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d485a7b",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
